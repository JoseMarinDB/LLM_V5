# main_generate.py â€“ genera todos los datasets jerÃ¡rquicos con lÃ³gica separada
import json, pathlib
import pandas as pd
from functions import *
from datetime import datetime
from llm import llm_invoke
from dotenv import load_dotenv
import os
import plotly.graph_objects as go
from google.cloud import bigquery
from google.oauth2 import service_account
import re

# â”€â”€â”€â”€â”€ ConfiguraciÃ³n â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
LOAD_DOTENV = load_dotenv()

# â”€â”€â”€â”€â”€ Funciones â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def make_llm_prompt(biz_json):
    """
    Devuelve el prompt con placeholders {{â€¦}} para que el LLM los complete
    usando el JSON que se entrega al final.
    """
    return f"""
Eres analista senior en quick-commerce. GenerÃ¡ un informe EXACTAMENTE en este formato Markdown:

## ğŸ“Œ Finance Insights
<!--CHART:LINE_AFV-->
- **AFV paÃ­s:** ${{overall.afv_act}}
- **Î”AFV total:** **${{overall.delta_afv}} ({{overall.delta_afv_pct}}%)**
- **Efectos:**
    - Basket Size: ${{overall.efecto_bs}}
    - AIV: ${{overall.efecto_aiv}}
    - Mix: ${{overall.efecto_mix}}

- **AnÃ¡lisis:** 
    Indica claramente quÃ© verticales aportaron mÃ¡s y cÃ³mo interactuaron los efectos de share y AFV interno.

    <!--CHART:BAR_VERTICALS-->

- **Cocinas clave:** 
    Resume en 2-3 frases quÃ© cocinas explican la caÃ­da o suba de AFV usando `cocinasdown` y `cocinasup` (nombres, Î”Share, AFV). No utilices cursivas ni negritas en esta frase.
    
    <!--CHART:SCATTER_COCINAS-->

- **AnÃ¡lisis de Usuarios:** 
    Explica {{incomesummary}}

    <!--CHART:BAR_INCOME-->

---

## ğŸ“Œ AnÃ¡lisis Detallado por Vertical
{{#each verticals}}
    ### ğŸ·ï¸ {{name}}

    | MÃ©trica | Valor |
    |---------|-------|
    | **AFV actual** | ${{afv_act}} |
    | **Î”AFV** | ${{delta_afv}} ({{delta_afv_pct}} %) |
    | **Impacto paÃ­s** | Efecto Share: ${{contrib_share}} Â· AFV interno: ${{contrib_afv}} Â· **Neto: ${{impacto_neto}}** |
    | **Efectos** | BS: ${{efecto_bs}} Â· AIV: ${{efecto_aiv}} Â· Mix: ${{efecto_mix}} |
    | **Incentivo p/orden** | ${{incent_act_pp}} (Î” {{delta_incent}}) |

    **AnÃ¡lisis:** Resume en una frase por quÃ© sube o baja usando los datos de la tabla (mÃ¡x. 35 palabras).

    <!--CHART:WF_{{name}}-->

{{/each}}

---

## ğŸ“Œ Segmentos con mayor cambio de share (â‰¥ Â±0,3 p.p.)

### ğŸ”¼ Suben
| Vertical | Segmento | Î”Share | ContribuciÃ³n ($) | Comentario |
|----------|----------|--------|------------------|------------|
{{#each segments_up}}
| {{vertical}} | {{name}} | {{delta_share}} | {{contrib}} | {{comment}} |
{{/each}}

### ğŸ”½ Bajan
| Vertical | Segmento | Î”Share | ContribuciÃ³n ($) | Comentario |
|----------|----------|--------|------------------|------------|
{{#each segments_down}}
| {{vertical}} | {{name}} | {{delta_share}} | {{contrib}} | {{comment}} |
{{/each}}

---

## ğŸ“Œ Cocinas con mayor cambio de share (â‰¥ Â±0,3 p.p.)

### ğŸ”¼ Suben
| Vertical | Cocina | Î”Share | AFV ($) |
|----------|--------|--------|---------|
{{#each cocinas_up}}
| {{vertical}} | {{name}} | {{delta_share}} | {{afv}} |
{{/each}}

### ğŸ”½ Bajan
| Vertical | Cocina | Î”Share | AFV ($) |
|----------|--------|--------|---------|
{{#each cocinas_down}}
| {{vertical}} | {{name}} | {{delta_share}} | {{afv}} |
{{/each}}

---

UsÃ¡ el siguiente JSON para completar estrictamente los campos:

```json
{json.dumps(biz_json, ensure_ascii=False, indent=2)}
"""


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ BigQuery â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run_bigquery_query(creds_path: str, query: str) -> pd.DataFrame:
    client = bigquery.Client(
        credentials=service_account.Credentials.from_service_account_file(creds_path)
    )
    job_config = bigquery.QueryJobConfig()
    job_config.use_query_cache = True

    return client.query(query, job_config=job_config).to_dataframe()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Limpieza de Markdown â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def clean_md_whitespace(md_text: str) -> str:
    # elimina cursivas y dobles espacios donde aparezca patrÃ³n _texto_
    md_text = re.sub(r'_([^_]+)_', r'\1', md_text)
    # normaliza espacios que hayan quedado pegados a parÃ©ntesis
    md_text = re.sub(r'\)\s*(?=[A-Za-z0-9])', ') ', md_text)
    return md_text


# â”€â”€â”€â”€â”€ ParÃ¡metros â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CRED_PATH  = pathlib.Path(os.getenv('GOOGLE_APPLICATION_CREDENTIALS'))
SQL_PATH   = pathlib.Path("segments_base.sql")
SQL_INCOME_PATH = pathlib.Path("segments_income.sql")
OUT_DIR    = pathlib.Path("out_data")
SQL_AFV_TREND_PATH = pathlib.Path("afv_timeline.sql")   # guarda tu query aquÃ­


# Calcular fechas FROM y TO

FROM_REF = '2025-01-01'
TO_REF   = '2025-01-31'
FROM_ACT = '2025-02-01'
TO_ACT   = '2025-02-28'
COUNTRY  = 'Uruguay'





# â”€â”€â”€â”€â”€ Cargar plantilla SQL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with open(SQL_PATH, "r", encoding="utf-8") as f:
    sql_template = f.read()
 
SQL_QUERY = sql_template.format(
    ref_from=FROM_REF,
    ref_to=TO_REF,
    act_from=FROM_ACT,
    act_to=TO_ACT,
    country=COUNTRY
)
# â”€â”€â”€â”€â”€ Cargar plantilla SQL de ingresos â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with open(SQL_INCOME_PATH, "r", encoding="utf-8") as f:
    sql_template_2 = f.read()

SQL_INCOME_QUERY= sql_template_2.format(
    ref_from=FROM_REF,
    ref_to=TO_REF,
    act_from=FROM_ACT,
    act_to=TO_ACT,
    country=COUNTRY,
    income=True
)

# â”€â”€â”€â”€â”€ Cargar plantilla SQL de tendencia AFV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with open(SQL_AFV_TREND_PATH, "r", encoding="utf-8") as f:
    sql_tl_template = f.read()

SQL_AFV_TREND = sql_tl_template.format(
    act_to=TO_ACT,          # Ãºltimo mes = mes â€œactualâ€
    num_total_months_to_display=12,
    country=COUNTRY
)

# â”€â”€â”€â”€â”€ Calcular dÃ­as de referencia y actividad â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def days_between(start, end):
    return (datetime.strptime(end, "%Y-%m-%d") - datetime.strptime(start, "%Y-%m-%d")).days + 1
REF_DAYS = days_between(FROM_REF, TO_REF)
ACT_DAYS = days_between(FROM_ACT, TO_ACT)

# â”€â”€â”€â”€â”€ Crear directorio de salida â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
OUT_DIR.mkdir(exist_ok=True)


# â”€â”€â”€â”€â”€ 1) Extrae datos base de BigQuery â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
df_base = run_bigquery_query(CRED_PATH, SQL_QUERY)

df_income = run_bigquery_query(CRED_PATH, SQL_INCOME_QUERY)

df_timeline = run_bigquery_query(CRED_PATH, SQL_AFV_TREND)
df_timeline["month_date"] = pd.to_datetime(df_timeline["month_date"])

# Guardar datos base en CSV
df_base.to_csv(OUT_DIR / "datos_base.csv", index=False)
df_income.to_csv(OUT_DIR / "datos_ingresos.csv", index=False)


# 2) Split por nivel ---------------------------------------------------------
nivel_map = {"OVERALL": "overall", "VERTICAL": "vertical",
             "SEGMENT": "segment", "COCINA": "cocina"}

dfs = {key: df_base[df_base["nivel1"] == lvl].copy()
       for lvl, key in nivel_map.items()}

# 3) Transformaciones comunes -----------------------------------------------
for key, df in dfs.items():
    dfs[key] = (df.pipe(calcular_metricas_basicas)
                  .pipe(calcular_deltas)
                  .pipe(calcular_efectos_afv)
                  .pipe(calcular_deltas_porcentaje))

# 4) Clasificar driver + relevancia en verticales ----------------------------
dfs["vertical"] = clasificar_driver_vertical(dfs["vertical"])

# 5) ResÃºmenes segment / cocina ---------------------------------------------
seg_json   = resumen_segment(dfs["segment"])
coc_json   = resumen_cocina(dfs["cocina"])
# 5.1) Ingresos por segmento y cocina ---------------------------------------
income_up   = df_income[df_income["dif_share_en_pp"] > 0]\
               .assign(delta=lambda d: "+"+d["dif_share_en_pp"].astype(str)+" p.p.")\
               .to_dict("records")
income_down = df_income[df_income["dif_share_en_pp"] < 0]\
               .assign(delta=lambda d: d["dif_share_en_pp"].astype(str)+" p.p.")\
               .to_dict("records")
income_summary = build_income_summary(df_income)


#GUARDAR TODOS LOS DATAFRAMES EN CSV
for key, df in dfs.items():
    df.to_csv(OUT_DIR / f"datos_{key}.csv", index=False)
    
    
# 6) Business JSON para el LLM ----------------------------------------------
biz_json = preparar_json_llm(
    df_overall = dfs["overall"],
    df_vertical= dfs["vertical"],
    df_segment = dfs["segment"],
    df_cocina  = dfs["cocina"]       # pasa los DF completos; la funciÃ³n internamente filtra
)
seg_up, seg_down = top_movers(dfs["segment"], "SEGMENT")
coc_up, coc_down = top_movers(dfs["cocina"],  "COCINA")

biz_json["segments_up"]   = seg_up
biz_json["segments_down"] = seg_down
biz_json["cocinasup"]    = coc_up
biz_json["cocinasdown"]  = coc_down
# Agregar ingresos por income
biz_json["incomeup"]   = income_up
biz_json["incomedown"] = income_down
biz_json["incomesummary"] = income_summary

#guardar biz_json en un archivo
(OUT_DIR / "business_data.json").write_text(
    json.dumps(biz_json, ensure_ascii=False, indent=2), encoding="utf-8"
)


# 7) Prompt + llamada --------------------------------------------------------
prompt = make_llm_prompt(biz_json)
#guardar el prompt en un archivo
(OUT_DIR / "prompt_llm.md").write_text(prompt, encoding="utf-8")
respuesta_llm, costo = llm_invoke(prompt)

if respuesta_llm:
    (OUT_DIR / "report_llm.md").write_text(respuesta_llm, encoding="utf-8")
    print(f"âœ… Reporte generado â€“ costo estimado USD {costo:.4f}")
else:
    print("âŒ No se pudo generar el anÃ¡lisis")

# 8) Guardar CSVs finales (opcional) -----------------------------------------
for key, df in dfs.items():
    df.to_csv(OUT_DIR / f"datos_{key}.csv", index=False)

ctx = {
    "afv_ref": dfs["overall"].iat[0, dfs["overall"].columns.get_loc("afv_ref")],
    "bs"     : dfs["overall"].iat[0, dfs["overall"].columns.get_loc("efecto_bs")],
    "aiv"    : dfs["overall"].iat[0, dfs["overall"].columns.get_loc("efecto_aiv")],
    "mix"    : dfs["overall"].iat[0, dfs["overall"].columns.get_loc("efecto_mix")],
    "verticals": dfs["vertical"],        # df completo
    "cocinas_movers": pd.DataFrame(coc_up + coc_down),
    "income_df": df_income,
    "afv_timeline": df_timeline
}

# â”€â”€â”€â”€â”€ Interfaz Streamlit para mostrar el reporte â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import re, streamlit as st
from viz_runtime import CHART_BUILDERS ,build_waterfall_vertical



for vert in ctx["verticals"]["nivel2"].unique():
    CHART_BUILDERS[f"WF_{vert}"] = (
        lambda v=vert: (lambda c: build_waterfall_vertical(c, v, pct_base=True))
    )()

st.set_page_config(page_title="AFV Dashboard", layout="centered")
st.markdown("## AnÃ¡lisis de AFV")

#pattern = r"<!--CHART:([A-Z_]+)-->"
pattern = r"<!--CHART:([^>]+)-->"      # todo lo que haya hasta -->

chunks  = re.split(pattern, respuesta_llm)

for i, chunk in enumerate(chunks):
    if i % 2 == 0:                         # texto markdown
        if chunk.strip():
            st.markdown(chunk, unsafe_allow_html=True)
    else:                                  # nombre del grÃ¡fico
        key = chunk.strip()
        builder = CHART_BUILDERS.get(key)
        if builder:
            fig = builder(ctx)
            st.plotly_chart(fig, use_container_width=True)
            if key == "SCATTER_COCINAS":
                with st.expander("Ver explicaciÃ³n del grÃ¡fico", expanded=False):
                    st.info("""
                    â€¢ Eje X: Î”Share (p.p.) â€” derecha sube, izquierda baja.  
                    â€¢ Eje Y: AFV actual ($).  
                    â€¢ Burbuja: impacto absoluto en Î”AFV paÃ­s (mÃ¡s grande = mÃ¡s importante).  
                    â€¢ Color: verde gana share / rojo pierde share.
                    """)
        else:
            st.warning(f"[Grafico {key} no registrado]")


with st.expander("Ver datos base"):
    # Mostrar los datos base
    st.markdown("### Datos Base")
    st.dataframe(df_base)

    # Mostrar los datos por nivel
    for key, df in dfs.items():
        st.markdown(f"### Datos {key.capitalize()}")
        st.dataframe(df)

    # Mostrar el JSON de negocio
    st.markdown("### JSON de Negocio")
    st.json(biz_json, expanded=True)

    # Mostrar los segmentos y cocinas con mayor cambio de share
    st.markdown("### Segmentos con Mayor Cambio de Share")
    st.markdown("#### Segmentos que Suben")
    st.dataframe(pd.DataFrame(seg_up))
    st.markdown("#### Segmentos que Bajan")
    st.dataframe(pd.DataFrame(seg_down))
    st.markdown("#### Cocinas que Suben")
    st.dataframe(pd.DataFrame(coc_up))
    st.markdown("#### Cocinas que Bajan")
    st.dataframe(pd.DataFrame(coc_down))


# â”€â”€â”€â”€â”€ Fin del script â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€



